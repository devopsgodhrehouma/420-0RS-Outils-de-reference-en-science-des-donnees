---
title: "Construction d'un R√©seau de Neurones - Les Bases"  
description: "Apprenez √† construire un r√©seau de neurones pas √† pas"  
tags: ["Deep Learning", "R√©seaux de Neurones", "Architecture", "Artificial Intelligence"]  
slug: "neural-network-construction"  
---

# Construction d'un R√©seau de Neurones - Les Bases

---

## Table des mati√®res

1. [Architecture d'un r√©seau de neurones](#architecture)
2. [Les diff√©rentes couches](#couches)
3. [La propagation avant](#propagation)
4. [L'apprentissage](#apprentissage)
5. [Les hyperparam√®tres](#hyperparametres)
6. [Impl√©mentation pratique : Construisons ensemble un r√©seau de neurones](#implementation)
7. [L'apprentissage](#apprentissage)


---

<a id="architecture"></a>  
<br/>

## 1. Architecture d'un r√©seau de neurones  
---
### 1.1. Vue d'ensemble  

Un r√©seau de neurones est une **collection organis√©e de neurones artificiels** qui travaillent ensemble pour r√©soudre un probl√®me sp√©cifique.  
- **Chaque neurone** traite une partie des donn√©es.  
- **Chaque couche** joue un r√¥le dans l'extraction ou la transformation de l'information.  
- **Le r√©seau dans son ensemble** apprend √† partir des donn√©es pour produire une sortie fiable.  

### 1.2. Les principes fondamentaux  

- **Organisation en couches** : Entr√©e ‚Üí Cach√©es ‚Üí Sortie.  
- **Connexions entre neurones** : Pond√©r√©es pour moduler l‚Äôimportance des signaux.  
- **Flux d‚Äôinformation unidirectionnel** : Dans un r√©seau classique, l‚Äôinformation circule de l‚Äôentr√©e √† la sortie sans retour imm√©diat.

:::info  
**M√©taphore** : Imaginez une cha√Æne de production.  
1. Les mati√®res premi√®res (couche d‚Äôentr√©e) sont transform√©es par chaque √©quipe (couches cach√©es).  
2. √Ä la fin, le produit fini est pr√™t (couche de sortie).  
:::  

[‚¨ÜÔ∏è Retour √† la table des mati√®res](#table-des-mati√®res)  

<a id="couches"></a>  
<br/>

---


## 2. Les diff√©rentes couches  
---

### 2.1. Couche d‚Äôentr√©e  

- **R√¥le principal** :  
  - Accepter les donn√©es brutes (ex. pixels, texte, donn√©es num√©riques).  
  - Les normaliser pour √©viter des √©carts excessifs.  
  - Distribuer ces donn√©es aux couches suivantes.  

### 2.2. Couches cach√©es  

- **Processus cl√©** :  
  - D√©tecter des motifs dans les donn√©es.  
  - Apprendre des repr√©sentations abstraites √† diff√©rents niveaux.  

:::info  
Prenons l'exemple d'une reconnaissance faciale¬†:  
- **Couche d‚Äôentr√©e** : Pixels de l‚Äôimage.  
- **Premi√®re couche cach√©e** : D√©tection des contours.  
- **Couches suivantes** : Identification des formes (nez, yeux, etc.).  
- **Derni√®re couche cach√©e** : Assemblage pour former un visage complet.  
:::  

### 2.3. Couche de sortie  

- Fournit la **pr√©diction finale**.  
- La sortie peut √™tre¬†:  
  - Une classification (probabilit√©s).  
  - Une valeur continue (r√©gression).  
  - Des cat√©gories (texte, labels).  


[‚¨ÜÔ∏è Retour √† la table des mati√®res](#table-des-mati√®res)  

<a id="propagation"></a>  
<br/>

---


## 3. La propagation avant  
---

### 3.1. Le flux d‚Äôinformation  

1. **Entr√©e des donn√©es**  
   - Les donn√©es sont transform√©es en un format num√©rique exploitable.  
   - Elles sont normalis√©es pour une meilleure stabilit√©.  

2. **Propagation √† travers les couches**  
   - Chaque neurone effectue un calcul simple :
     - Il multiplie chaque entr√©e par son poids
     - Il additionne tous ces r√©sultats
     - Il ajoute un biais
   
   Par exemple, avec 2 entr√©es :
   - Entr√©e 1 √ó Poids 1 = R√©sultat 1
   - Entr√©e 2 √ó Poids 2 = R√©sultat 2  
   - Total = R√©sultat 1 + R√©sultat 2 + Biais

<br/>
    # ‚ûó √âquation math√©matique :
    $$z = w_1x_1 + w_2x_2 + b$$

   - Application d‚Äôune fonction d‚Äôactivation pour produire une sortie.  

3. **G√©n√©ration de la sortie**  
   - Les couches finales produisent la sortie dans un format adapt√© au probl√®me.  

:::info  
**M√©taphore** : C'est comme une rivi√®re qui coule d‚Äôamont en aval¬†:  
- Les donn√©es (eau) passent par des filtres successifs (couches).  
- √Ä la fin, vous obtenez une eau pure (pr√©diction).  
:::  


[‚¨ÜÔ∏è Retour √† la table des mati√®res](#table-des-mati√®res)  

<a id="apprentissage"></a> 
<br/>

---

## 4. L'apprentissage
---

### 4.1. Principe g√©n√©ral

L'apprentissage est un processus it√©ratif qui ajuste les **poids** et **biais** du r√©seau pour minimiser une erreur mesur√©e par une fonction de perte. C'est le c≈ìur de l'intelligence artificielle.

### 4.2. Les trois phases cl√©s

1. **Propagation avant (Forward Pass)**
   - Transmission des donn√©es d'entr√©e √† travers le r√©seau
   - Calcul des pr√©dictions couche par couche
   - Obtention d'une sortie finale √† comparer avec la cible

2. **Calcul de l'erreur (Loss Computation)**
   - √âvaluation pr√©cise de l'√©cart entre pr√©diction et r√©alit√©
   - Utilisation de fonctions de perte adapt√©es au probl√®me :
     - MSE pour la r√©gression
     - Cross-entropy pour la classification
     - Autres fonctions sp√©cialis√©es selon les besoins

3. **Optimisation (Backward Pass)**
   - Calcul des gradients par r√©tropropagation
   - Mise √† jour intelligente des param√®tres via l'optimiseur
   - Algorithmes courants : SGD, Adam, RMSprop

### 4.3. Analogies et visualisation

:::info
üéØ **M√©taphore du tir √† l'arc** :
- La propagation avant = d√©cocher la fl√®che
- L'erreur = distance √† la cible 
- L'optimisation = ajustement de la posture et de la vis√©e

üö≤ **M√©taphore du v√©lo** :
- Chaque tentative = une propagation avant
- Les chutes = les erreurs
- L'apprentissage de l'√©quilibre = l'optimisation des param√®tres
:::

### 4.4. Bonnes pratiques

- **Initialisation** : D√©marrer avec des poids appropri√©s
- **Batch size** : √âquilibrer vitesse et stabilit√©
- **Learning rate** : Ajuster selon la convergence
- **Monitoring** : Suivre l'√©volution de la perte


:::info
**Analogies concr√®tes pour les hyperparam√®tres** :

üé™ **Batch size (taille des lots)** :
- Imaginez une file d'attente de 1000 personnes pour un man√®ge
- Vous pouvez faire entrer :
  - 20 personnes ‚Üí 50 tours n√©cessaires (plus stable mais plus long)
  - 100 personnes ‚Üí 10 tours n√©cessaires (plus rapide mais plus chaotique)
  - 1000 personnes ‚Üí 1 seul tour (rapide mais tr√®s instable)

‚ö° **Learning rate (taux d'apprentissage)** :
- C'est comme apprendre √† faire du v√©lo :
  - Trop prudent (0.0001) ‚Üí progression tr√®s lente
  - Trop t√©m√©raire (0.1) ‚Üí chutes fr√©quentes
  - Bien dos√© (0.01) ‚Üí apprentissage optimal

üéØ **Nombre d'epochs (cycles d'entra√Ænement)** :
- Comme apprendre une chor√©graphie :
  - Trop peu ‚Üí mouvements impr√©cis
  - Trop ‚Üí risque de sur-apprentissage
  - Juste assez ‚Üí ma√Ætrise parfaite

üßÆ **Taille du r√©seau** :
- Comme une √©quipe de travail :
  - Trop petite ‚Üí ne peut pas g√©rer la complexit√©
  - Trop grande ‚Üí gaspillage de ressources
  - Bien dimensionn√©e ‚Üí efficacit√© maximale
:::



:::info  
**M√©taphore** : Apprendre √† lancer une fl√©chette.  
- Vous lancez (propagation avant).  
- Vous ratez la cible (erreur).  
- Vous ajustez votre position (r√©tropropagation).  
- Vous essayez √† nouveau (nouvelle mise √† jour).  
:::  

[‚¨ÜÔ∏è Retour √† la table des mati√®res](#table-des-mati√®res)  
<a id="hyperparametres"></a> 

<br/>

---

 
## 5. Les hyperparam√®tres  
---

### 5.1. Param√®tres cl√©s  

1. **Nombre de couches**  
   - Plus de couches = capacit√© √† apprendre des motifs complexes.  

2. **Nombre de neurones par couche**  
   - Influence la largeur du r√©seau et sa capacit√© √† mod√©liser les donn√©es.  

3. **Taux d‚Äôapprentissage**  
   - Contr√¥le la vitesse d‚Äôapprentissage :  
     - Trop grand¬†: le mod√®le peut diverger.  
     - Trop petit¬†: l‚Äôapprentissage sera lent.  

:::info
**Les hyperparam√®tres : des choix AVANT l'action !**

Imaginez la pr√©paration d'un g√¢teau :
- AVANT de commencer, vous devez choisir :
  - La taille du moule (architecture du r√©seau)
  - La temp√©rature du four (taux d'apprentissage)
  - Le temps de cuisson (nombre d'epochs)

Une fois la cuisson lanc√©e, impossible de changer ces param√®tres !

C'est la m√™me chose pour un r√©seau de neurones :
- **Le taux d'apprentissage** doit √™tre fix√© avant :
  - Trop √©lev√© ‚Üí apprentissage chaotique
  - Bien dos√© ‚Üí apprentissage harmonieux

- **L'architecture** est d√©cid√©e en amont :
  - Trop de neurones ‚Üí ressources gaspill√©es
  - Trop peu ‚Üí capacit√© insuffisante

‚ùó **Important** : Comme pour une recette, ces choix sont cruciaux et ne peuvent pas √™tre modifi√©s pendant l'ex√©cution. Il faut tout recommencer si les param√®tres ne conviennent pas !
:::

[‚¨ÜÔ∏è Retour √† la table des mati√®res](#table-des-mati√®res)  

<a id="implementation"></a>  
<br/>

---


## 6. Impl√©mentation pratique : Construisons ensemble un r√©seau de neurones  
--- 

### 6.1. Pourquoi observer l'architecture ?

Dans ce paragraphe, nous allons examiner attentivement l'architecture d'un r√©seau de neurones simple. Je vous invite particuli√®rement √† observer :

- Le **nombre de neurones** dans chaque couche
- L'**organisation des couches** (entr√©e, cach√©e, sortie)

Ne vous inqui√©tez pas si tous les concepts ne sont pas clairs imm√©diatement. L'important est d'abord de bien visualiser comment les neurones sont organis√©s et connect√©s entre eux.

:::tip Rassurez-vous !
Ne vous inqui√©tez pas des fonctions complexes que vous verrez dans le code - elles sont d√©j√† construites par d'autres d√©veloppeurs. Concentrons-nous plut√¥t sur le concept essentiel :

Imaginez un entonnoir intelligent qui :
1. Re√ßoit beaucoup d'informations en entr√©e
2. Les filtre et les simplifie couche apr√®s couche
3. Arrive √† une d√©cision finale simple

Par exemple :
- Entr√©e : 1000 pixels d'une image
- Premi√®re couche cach√©e : 100 neurones qui d√©tectent des formes basiques
- Deuxi√®me couche : 10 neurones qui combinent ces formes
- Sortie : 1 neurone qui dit "chat" ou "pas chat"

C'est la magie des r√©seaux de neurones : transformer la complexit√© en simplicit√©, √©tape par √©tape !
:::



### 6.2. Exemple de code : Une seule couche cach√©e  

Commen√ßons avec le r√©seau le plus simple possible, une **seule couche cach√©e**, pour que vous puissiez vous concentrer sur les bases.

```python
import torch
import torch.nn as nn

# √âtape 1 : D√©finir un r√©seau avec une seule couche cach√©e
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.hidden = nn.Linear(3, 5)  # 3 entr√©es, 5 neurones dans la couche cach√©e
        self.output = nn.Linear(5, 2) # 5 neurones cach√©s, 2 sorties

    def forward(self, x):
        x = torch.relu(self.hidden(x))  # Fonction d'activation ReLU
        return self.output(x)

# √âtape 2 : Initialiser le r√©seau et les donn√©es
model = SimpleNN()
inputs = torch.tensor([[1.0, 2.0, 3.0]])  # Exemple de 3 entr√©es (ex : poids, taille, √¢ge)
outputs = model(inputs)  # Calcul de la sortie

print(f"Sortie du r√©seau : {outputs}")
```

#### Ce que vous venez de faire :
1. Vous avez **d√©fini un r√©seau** avec 3 entr√©es, une couche cach√©e de 5 neurones, et une sortie de 2 neurones.
2. Vous avez pass√© des donn√©es d‚Äôentr√©e et observ√© une **sortie non entra√Æn√©e**. C‚Äôest normal, nous n‚Äôavons pas encore entra√Æn√© le mod√®le !



### 6.3. Sch√©matisation du r√©seau

Visualisez le r√©seau comme une cha√Æne :

```
[Entr√©e] (3 neurones) --> [Couche Cach√©e] (5 neurones, ReLU) --> [Sortie] (2 neurones)
```
```
[Entr√©e]           [Couche Cach√©e]         [Sortie]
   x‚ÇÅ ----‚Üí         o                        y‚ÇÅ
   x‚ÇÇ ----‚Üí        ooo        ReLU  ----‚Üí   y‚ÇÇ
   x‚ÇÉ ----‚Üí         o                             
```

- **Entr√©e (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ)** : Repr√©sente les donn√©es que vous voulez analyser.  
- **Couche Cach√©e (5 neurones)** : Apprend des motifs complexes gr√¢ce √† ses connexions pond√©r√©es.  
- **Sortie (y‚ÇÅ, y‚ÇÇ)** : Fournit les r√©sultats (ex. classification ou pr√©diction).



### 6.4. Ajout d‚Äôune fonction de perte et optimisation  

Pour que notre r√©seau apprenne, nous devons :
1. **Calculer une erreur** (ou perte) entre la sortie du r√©seau et les valeurs attendues.  
2. **Ajuster les poids** du r√©seau pour r√©duire cette erreur.  

```python
import torch.optim as optim

# √âtape 1 : D√©finir la fonction de perte et l‚Äôoptimiseur
criterion = nn.MSELoss()  # Erreur quadratique moyenne (pour des sorties continues)
optimizer = optim.SGD(model.parameters(), lr=0.01)  # Descente de gradient avec un faible taux d'apprentissage

# √âtape 2 : D√©finir les cibles
targets = torch.tensor([[0.0, 1.0]])  # Exemple de cibles (valeurs attendues)

# √âtape 3 : Propagation avant, calcul de la perte et r√©tropropagation
outputs = model(inputs)               # Calcul des sorties
loss = criterion(outputs, targets)   # Calcul de la perte
loss.backward()                       # Calcul des gradients
optimizer.step()                      # Mise √† jour des poids

print(f"Perte apr√®s une √©tape d'apprentissage : {loss.item():.4f}")
```



### 6.5. Ajout d‚Äôune deuxi√®me couche cach√©e  

Une fois que vous √™tes √† l‚Äôaise avec une seule couche, passons √† **deux couches cach√©es**. 

#### Pourquoi ajouter une deuxi√®me couche cach√©e ?
- Cela permet au r√©seau d‚Äôapprendre des **relations plus complexes** dans vos donn√©es.
- Chaque couche cach√©e extrait des **motifs de plus haut niveau**.

```python
class AdvancedNN(nn.Module):
    def __init__(self):
        super(AdvancedNN, self).__init__()
        self.hidden1 = nn.Linear(3, 5)  # Premi√®re couche cach√©e (5 neurones)
        self.hidden2 = nn.Linear(5, 4)  # Deuxi√®me couche cach√©e (4 neurones)
        self.output = nn.Linear(4, 2)   # Couche de sortie (2 neurones)

    def forward(self, x):
        x = torch.relu(self.hidden1(x)) # Activation apr√®s la premi√®re couche
        x = torch.relu(self.hidden2(x)) # Activation apr√®s la deuxi√®me couche
        return self.output(x)

# Initialisation
model = AdvancedNN()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
```



### 6.6. Exercice : Une troisi√®me couche cach√©e  

#### Pourquoi une troisi√®me couche ?
- Une troisi√®me couche cach√©e permet de **mod√©liser des interactions encore plus complexes**.
- Par exemple, si vos donn√©es repr√©sentent des images, cette couche pourrait apprendre des formes comme des yeux ou des motifs g√©om√©triques.

#### Sp√©cifications :
- Couche cach√©e 1 : 6 neurones.  
- Couche cach√©e 2 : 4 neurones.  
- Couche cach√©e 3 : 3 neurones.  
- Couche de sortie : 2 neurones.

```python
class ThreeLayerNN(nn.Module):
    def __init__(self):
        super(ThreeLayerNN, self).__init__()
        self.hidden1 = nn.Linear(3, 6)  # Premi√®re couche cach√©e (6 neurones)
        self.hidden2 = nn.Linear(6, 4)  # Deuxi√®me couche cach√©e (4 neurones)
        self.hidden3 = nn.Linear(4, 3)  # Troisi√®me couche cach√©e (3 neurones)
        self.output = nn.Linear(3, 2)   # Couche de sortie (2 neurones)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = torch.relu(self.hidden3(x))
        return self.output(x)

# Initialisation
model = ThreeLayerNN()
criterion = nn.CrossEntropyLoss()  # Pour des probl√®mes de classification
optimizer = optim.Adam(model.parameters(), lr=0.005)
```



### 6.7. R√©sum√© 

- **Une seule couche cach√©e** : Comprendre la structure de base d‚Äôun r√©seau.  
- **Deux couches cach√©es** : Ajouter de la complexit√© et capturer des motifs plus subtils.  
- **Trois couches cach√©es ou plus** : Mod√©liser des relations complexes et pr√©parer des architectures profondes.  


[‚¨ÜÔ∏è Retour √† la table des mati√®res](#table-des-mati√®res)

<br/>



