---
title: "Impl√©mentation et Analyse d'un R√©seau de Neurones avec PyTorch"
description: "Analyse d√©taill√©e d'une impl√©mentation de r√©seau de neurones avec PyTorch pour la pr√©diction des co√ªts de production"
tags: ["Deep Learning", "PyTorch", "R√©gression", "Analyse de Code"]
slug: /06-Deep-Learning/Chapter6-Practice2-PyTorch/01-Code-Analysis
---

# Exemple pratique 1 : Impl√©mentation et Analyse d'un R√©seau de Neurones avec PyTorch

---

## Table des mati√®res

1. [Introduction](#1---introduction)
2. [Contexte du Probl√®me √† R√©soudre](#2---contexte-du-probl√®me-√†-r√©soudre--pr√©diction-des-co√ªts-de-production-dun-produit)
3. [Pr√©paration des Donn√©es](#3---pr√©paration-des-donn√©es)
4. [Architecture du Mod√®le](#4---architecture-du-mod√®le)
5. [Points Cl√©s](#5---points-cl√©s)
6. [R√©sum√©](#6---r√©sum√©)
7. [Annexe : Deuxi√®me Exemple de Code](#7---annexe--un-deuxi√®me-exemple-de-code-pour-pr√©diction-des-co√ªts-avec-un-mod√®le-profond-5-couches)
8. [Caract√©ristiques du Mod√®le](#8---caract√©ristiques-du-mod√®le)
9. [Points Cl√©s](#9---points-cl√©s)
10. [R√©f√©rences](#10---r√©f√©rences)


<br/>
---
## Objectifs d'apprentissage
---

- Comprendre l'impl√©mentation d'un r√©seau de neurones avec PyTorch
- Ma√Ætriser les concepts fondamentaux de l'apprentissage profond
- D√©velopper des comp√©tences pratiques en programmation PyTorch


[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>
---
## 1 - Introduction
---

Dans ce chapitre, nous allons analyser en d√©tail une impl√©mentation pratique d'un r√©seau de neurones avec PyTorch. Notre objectif est de cr√©er un mod√®le capable de pr√©dire les co√ªts de production d'un produit en fonction de diff√©rentes variables d'entr√©e.

Cette analyse pas √† pas nous permettra de :
- Comprendre comment structurer un projet PyTorch complet
- Ma√Ætriser les concepts cl√©s comme les DataLoaders et les Datasets
- Apprendre √† impl√©menter et entra√Æner un mod√®le de r√©gression
- Visualiser et interpr√©ter les r√©sultats

Le code que nous allons √©tudier utilise des donn√©es simul√©es pour se concentrer sur les aspects techniques plut√¥t que sur la qualit√© des donn√©es. Cette approche nous permet de nous familiariser avec PyTorch dans un environnement contr√¥l√© avant de passer √† des donn√©es r√©elles plus complexes.


:::info
üëã **Note aux √©tudiants**

Dans cette section, nous n'allons pas ex√©cuter le code directement. L'objectif est plut√¥t de comprendre en profondeur chaque composant du code et son r√¥le dans la cr√©ation d'un r√©seau de neurones avec PyTorch. Nous allons analyser :

- Comment les donn√©es sont pr√©par√©es et structur√©es
- La logique derri√®re chaque classe et fonction
- Le r√¥le des diff√©rents composants (DataLoader, Dataset, mod√®le, etc.)
- Le processus d'entra√Ænement et d'√©valuation
- Les techniques de visualisation des r√©sultats

Cette approche analytique vous permettra de mieux ma√Ætriser les concepts fondamentaux et de pouvoir adapter ce code √† vos propres projets futurs.
:::



[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 2 - Contexte du Probl√®me √† R√©soudre : Pr√©diction des Co√ªts de Production d'un Produit
---

Vous √™tes charg√© de d√©velopper un mod√®le de r√©seau de neurones capable de pr√©dire les co√ªts de production d'un produit en fonction de plusieurs variables. Les donn√©es incluent :
- La quantit√© de mati√®re premi√®re utilis√©e (en kg),
- Le temps de production (en heures),
- Le co√ªt de la main-d'≈ìuvre (en $).

L'objectif est de former un mod√®le qui minimise l'erreur entre les co√ªts r√©els et pr√©dits. 

Nous demandons √† vous de :
1. Comprendre la structure d'un r√©seau de neurones en PyTorch.
2. Travailler avec des DataLoaders pour manipuler des ensembles de donn√©es.
3. Impl√©menter les fonctions d'entra√Ænement et d'√©valuation.
4. Visualiser les performances du mod√®le.


[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 3 - √âtapes du Code
---

- √âtape 1 : Importation des Biblioth√®ques
- √âtape 2 : G√©n√©ration de donn√©es simul√©es
- √âtape 3 : S√©paration des donn√©es
- √âtape 4 : Cr√©ation des datasets
- √âtape 5 : D√©finition du mod√®le
- √âtape 6 : Configuration de l'optimiseur et de la fonction de co√ªt
- √âtape 7 : Fonction d'entra√Ænement
- √âtape 8 : Fonction d'√©valuation
- √âtape 9 : Entra√Ænement du mod√®le
- √âtape 10 : Visualisation des performances
- √âtape 11 : √âvaluation finale



[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 4 - Code Complet
---

```python
import torch
from torch import nn, optim, utils, autograd
import numpy as np
import matplotlib.pyplot as plt

# G√©n√©ration de donn√©es simul√©es
np.random.seed(42)
torch.manual_seed(42)

# Variables : quantit√©, temps, co√ªt main d'≈ìuvre
X_data = np.random.rand(1000, 3) * 10  # 1000 √©chantillons avec 3 variables
y_data = 3 * X_data[:, 0] + 2 * X_data[:, 1] + 4 * X_data[:, 2] + np.random.randn(1000) * 2

# S√©paration des donn√©es
train_size = int(0.8 * len(X_data))
X_train, X_test = X_data[:train_size], X_data[train_size:]
y_train, y_test = y_data[:train_size], y_data[train_size:]

# Cr√©ation des datasets
class ProductionDataset(utils.data.Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Ajouter une dimension

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = ProductionDataset(X_train, y_train)
test_dataset = ProductionDataset(X_test, y_test)

train_loader = utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = utils.data.DataLoader(test_dataset, batch_size=32)

# D√©finition du mod√®le
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.model(x)

model = NeuralNetwork()

# Configuration de l'optimiseur et de la fonction de co√ªt
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

# Fonction d'entra√Ænement
def train_model(dataloader, model, loss_fn, optimizer):
    model.train()
    total_loss = 0
    for X_batch, y_batch in dataloader:
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = loss_fn(predictions, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

# Fonction d'√©valuation
def evaluate_model(dataloader, model, loss_fn):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            predictions = model(X_batch)
            loss = loss_fn(predictions, y_batch)
            total_loss += loss.item()
    return total_loss / len(dataloader)

# Entra√Ænement du mod√®le
num_epochs = 50
train_losses = []
test_losses = []

for epoch in range(num_epochs):
    train_loss = train_model(train_loader, model, loss_fn, optimizer)
    test_loss = evaluate_model(test_loader, model, loss_fn)
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    print(f"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}")

# Visualisation des pertes
plt.figure(figsize=(10, 5))
plt.plot(range(num_epochs), train_losses, label="Train Loss")
plt.plot(range(num_epochs), test_losses, label="Test Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss during Training and Testing")
plt.legend()
plt.show()

# √âvaluation finale
model.eval()
X_sample = torch.tensor(X_test[:5], dtype=torch.float32)
y_sample = y_test[:5]

predictions = model(X_sample).detach().numpy()
for i, (real, pred) in enumerate(zip(y_sample, predictions)):
    print(f"Sample {i + 1}: Real Value = {real:.2f}, Predicted Value = {pred[0]:.2f}")
```


[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 5 - Points Cl√©s
---

1. **Manipulation des Datasets et DataLoaders** : Charger, normaliser et pr√©parer des donn√©es pour l'entra√Ænement.
2. **D√©finition d'un Mod√®le** : Cr√©er un mod√®le avec `nn.Sequential`, ajouter des activations non lin√©aires.
3. **Fonction de Co√ªt et Optimiseur** : Utiliser des fonctions comme `MSELoss` et optimiser avec `Adam`.
4. **Entra√Ænement et √âvaluation** : Impl√©menter des fonctions d'entra√Ænement et d'√©valuation robustes.
5. **Visualisation des Performances** : Utiliser des graphiques pour analyser les pertes et performances du mod√®le.

Ce code couvre tous les aspects n√©cessaires pour apprendre la mod√©lisation avec PyTorch tout en r√©solvant un probl√®me pratique.

[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 6 - R√©sum√© 
---

Dans ce chapitre, nous avons explor√© en d√©tail l'impl√©mentation d'un r√©seau de neurones avec PyTorch pour pr√©dire les co√ªts de production. Les points essentiels abord√©s sont :

1. **Pr√©paration des Donn√©es**
   - G√©n√©ration de donn√©es simul√©es pour l'entra√Ænement
   - S√©paration en ensembles d'entra√Ænement et de test
   - Utilisation des DataLoaders pour une gestion efficace des lots

2. **Architecture du Mod√®le**
   - Cr√©ation d'un r√©seau de neurones multicouches
   - Configuration des couches et des fonctions d'activation
   - Choix des hyperparam√®tres appropri√©s

3. **Processus d'Entra√Ænement**
   - Impl√©mentation des fonctions d'entra√Ænement et d'√©valuation
   - Utilisation de l'optimiseur Adam et de la fonction de perte MSE
   - Suivi de la progression avec les m√©triques de perte

4. **√âvaluation et Visualisation**
   - Analyse des courbes d'apprentissage
   - Comparaison des performances sur les ensembles d'entra√Ænement et de test
   - Validation des pr√©dictions sur des √©chantillons sp√©cifiques

Cette approche pratique permet de comprendre les concepts fondamentaux de PyTorch tout en r√©solvant un probl√®me concret de r√©gression.


[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 7 - Annexe : un deuxi√®me exemple de code pour pr√©diction des Co√ªts avec un Mod√®le Profond (5 Couches)
---

Le mod√®le sera configur√© avec :
- **5 couches** enti√®rement connect√©es (Dense).
- Nombre de neurones par couche : [3 (entr√©e), 128, 64, 32, 16, 1 (sortie)].
- Fonction d'activation : `ReLU` pour les couches cach√©es.



#### Code Complet

```python
import torch
from torch import nn, optim, utils
import numpy as np
import matplotlib.pyplot as plt

# ---------------------------------------------------
# G√©n√©ration de donn√©es simul√©es
# ---------------------------------------------------
np.random.seed(42)
torch.manual_seed(42)

# Donn√©es : quantit√©, temps, co√ªt main d'≈ìuvre
X_data = np.random.rand(5000, 3) * 10  # 5000 √©chantillons avec 3 variables
y_data = 3 * X_data[:, 0] + 2 * X_data[:, 1] + 4 * X_data[:, 2] + np.random.randn(5000) * 2

# S√©paration des donn√©es (80% train, 20% test)
train_size = int(0.8 * len(X_data))
X_train, X_test = X_data[:train_size], X_data[train_size:]
y_train, y_test = y_data[:train_size], y_data[train_size:]

# ---------------------------------------------------
# Cr√©ation du Dataset et DataLoader
# ---------------------------------------------------
class ProductionDataset(utils.data.Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Ajouter une dimension pour la sortie

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = ProductionDataset(X_train, y_train)
test_dataset = ProductionDataset(X_test, y_test)

train_loader = utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = utils.data.DataLoader(test_dataset, batch_size=64)

# ---------------------------------------------------
# D√©finition du mod√®le (5 couches)
# ---------------------------------------------------
class DeepNeuralNetwork(nn.Module):
    def __init__(self):
        super(DeepNeuralNetwork, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(3, 128),   # Entr√©e : 3 -> 128 neurones
            nn.ReLU(),
            nn.Linear(128, 64),  # Couche 2 : 128 -> 64 neurones
            nn.ReLU(),
            nn.Linear(64, 32),   # Couche 3 : 64 -> 32 neurones
            nn.ReLU(),
            nn.Linear(32, 16),   # Couche 4 : 32 -> 16 neurones
            nn.ReLU(),
            nn.Linear(16, 1)     # Couche de sortie : 16 -> 1 neurone
        )

    def forward(self, x):
        return self.model(x)

model = DeepNeuralNetwork()

# ---------------------------------------------------
# Configuration de l'optimiseur et de la fonction de co√ªt
# ---------------------------------------------------
optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

# ---------------------------------------------------
# Fonctions d'entra√Ænement et d'√©valuation
# ---------------------------------------------------
def train_model(dataloader, model, loss_fn, optimizer):
    model.train()
    total_loss = 0
    for X_batch, y_batch in dataloader:
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = loss_fn(predictions, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate_model(dataloader, model, loss_fn):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            predictions = model(X_batch)
            loss = loss_fn(predictions, y_batch)
            total_loss += loss.item()
    return total_loss / len(dataloader)

# ---------------------------------------------------
# Entra√Ænement du mod√®le
# ---------------------------------------------------
num_epochs = 100
train_losses = []
test_losses = []

for epoch in range(num_epochs):
    train_loss = train_model(train_loader, model, loss_fn, optimizer)
    test_loss = evaluate_model(test_loader, model, loss_fn)
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    print(f"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}")

# ---------------------------------------------------
# Visualisation des performances
# ---------------------------------------------------
plt.figure(figsize=(10, 5))
plt.plot(range(num_epochs), train_losses, label="Train Loss")
plt.plot(range(num_epochs), test_losses, label="Test Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss during Training and Testing")
plt.legend()
plt.show()

# ---------------------------------------------------
# √âvaluation finale avec des √©chantillons
# ---------------------------------------------------
model.eval()
X_sample = torch.tensor(X_test[:10], dtype=torch.float32)
y_sample = y_test[:10]

predictions = model(X_sample).detach().numpy()
print("\nComparaison entre les valeurs r√©elles et les pr√©dictions :")
print("Valeur r√©elle\tPr√©diction\tErreur")
print("------------------------------------")
for i, (real, pred) in enumerate(zip(y_sample, predictions)):
    print(f"{real:.2f}\t\t{pred[0]:.2f}\t\t{abs(real - pred[0]):.2f}")
```



[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 8 - Caract√©ristiques du Mod√®le
---

1. **Architecture Profonde (5 Couches)** : Les 5 couches permettent au mod√®le de capturer des relations complexes dans les donn√©es.
   - **Entr√©e** : 3 caract√©ristiques (quantit√©, temps, co√ªt).
   - **Couche cach√©e 1** : 128 neurones.
   - **Couche cach√©e 2** : 64 neurones.
   - **Couche cach√©e 3** : 32 neurones.
   - **Couche cach√©e 4** : 16 neurones.
   - **Sortie** : 1 neurone (valeur de co√ªt pr√©dite).

2. **Optimiseur** : `Adam` est utilis√© pour une convergence rapide et efficace.
3. **Fonction de co√ªt** : `MSELoss` (erreur quadratique moyenne) pour minimiser la diff√©rence entre les valeurs r√©elles et pr√©dites.


[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 9 - Points Cl√©s
---

1. Comprendre comment augmenter la complexit√© d'un r√©seau de neurones avec plusieurs couches.
2. Apprendre √† √©quilibrer le nombre de neurones par couche pour √©viter le surajustement.
3. Visualiser les performances en tra√ßant les pertes pour l'ensemble d'entra√Ænement et de test.
4. Comparer les pr√©dictions du mod√®le avec les valeurs r√©elles pour √©valuer la pr√©cision.


[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
<br/>

---
## 10 - R√©f√©rences
---

- [Getting Started](https://www.kaggle.com/discussions/getting-started/114357)
- [Pipeline for every PyTorch image classification problem](https://medium.com/@siromermer/pipeline-for-every-pytorch-image-classification-problem-creating-dataset-f0f57d6ae225)
- [PyTorch tutorial beginner to advance](https://www.kaggle.com/code/aravindanr22052001/pytorch-tutorial-beginner-to-advance)
- [CIFAR10 tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)

<br/>
[‚¨ÜÔ∏è Retour √† la Table des Mati√®res](#table-des-mati√®res)
